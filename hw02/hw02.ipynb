{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine-maniac \n",
      "last updated: 2020-07-21 \n",
      "\n",
      "CPython 3.7.4\n",
      "IPython 7.8.0\n",
      "\n",
      "numpy 1.16.5\n",
      "scipy 1.3.1\n",
      "matplotlib 3.1.1\n",
      "sklearn 0.21.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r'c:\\users\\USERNAME\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages')\n",
    "%load_ext watermark\n",
    "%watermark -d -u -a 'machine-maniac' -v -p numpy,scipy,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0]), 1: array([1]), 2: array([2])}\n",
      "{0: array([1, 3, 4, 6]), 1: array([0, 2, 5])}\n",
      "{0: array([1, 4]), 1: array([0, 5, 6]), 2: array([3]), 3: array([2])}\n"
     ]
    }
   ],
   "source": [
    "#  1.1.1 1.1) Splitting a node (10 pts)\n",
    "def split(array):\n",
    "    frequency_dict = {}\n",
    "    for i in range(0, len(array)):\n",
    "        if array[i] in frequency_dict:\n",
    "            frequency_dict[array[i]] = np.append(frequency_dict[array[i]], [i])\n",
    "        else:\n",
    "            frequency_dict[array[i]] = np.array([i])\n",
    "\n",
    "    sorted_dict = {}\n",
    "    for keys in sorted(frequency_dict.keys()):\n",
    "        sorted_dict[keys] = frequency_dict[keys]\n",
    "    return sorted_dict\n",
    "\n",
    "print (split(np.array([0,1,2])))\n",
    "print (split(np.array([1, 0, 1, 0, 0, 1, 0])))\n",
    "print (split(np.array([1, 0, 3, 2, 0, 1, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.4395\n",
      "0.0\n",
      "1.6577\n"
     ]
    }
   ],
   "source": [
    "# 1.1.2 1.2) Implement Entropy (10 pts)\n",
    "import math\n",
    "def entropy(array):\n",
    "    frequency_dict = split(array) \n",
    "    length_of_array = len(array)   \n",
    "    frequency_array = []\n",
    "    entropy = 0\n",
    "\n",
    "    for i in range(0, len(frequency_dict)):\n",
    "        frequency_array.append(len(list(frequency_dict.values())[i]))\n",
    "\n",
    "    for i in range(0, len(frequency_array)):\n",
    "        element_frequency = frequency_array[i]/length_of_array\n",
    "        entropy = entropy + (element_frequency * math.log2(element_frequency))\n",
    "    \n",
    "    if entropy != 0:\n",
    "        return entropy * -1\n",
    "\n",
    "    return entropy\n",
    "\n",
    "print(round(entropy(np.array([0, 1, 0, 1, 1, 0])), 4))\n",
    "print(round(entropy(np.array([1, 2])), 4))\n",
    "print(round(entropy(np.array([1, 1])), 4))\n",
    "print(round(entropy(np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), 4))\n",
    "print(round(entropy(np.array([0, 0, 0])), 4))\n",
    "print(round(entropy(np.array([1, 1, 1, 0, 1, 4, 4, 2, 1])), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4591\n",
      "0.2516\n"
     ]
    }
   ],
   "source": [
    "# 1.1.3 1.3) Implement Information Gain (10 pts)\n",
    "def information_gain(x_array, y_array):\n",
    "    parent_entropy = entropy(x_array)\n",
    "\n",
    "    split_dict = split(y_array).values()\n",
    "    \n",
    "    for val in split_dict:\n",
    "        freq = len(val)/len(x_array)\n",
    "        child_entropy = entropy([x_array[i] for i in val])\n",
    "        parent_entropy -= (child_entropy * freq)\n",
    "        \n",
    "    return parent_entropy\n",
    "\n",
    "\n",
    "x = np.array([0, 1, 0, 1, 0, 1])\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "print(round(information_gain(x, y), 4)) #0.4591\n",
    "\n",
    "x = np.array([0, 0, 1, 1, 2, 2])\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "print(round(information_gain(x, y), 4)) #0.2516\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [2 1]]\n",
      "\n",
      "Labels:\n",
      " [0 1 0 1 1 1]\n",
      "\n",
      "Decision tree:\n",
      " {'X_1 = 0': {'X_0 = 0': array([0]), 'X_0 = 1': array([0]), 'X_0 = 2': array([1])}, 'X_1 = 1': array([1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# 1.1.4 1.4) Decision Tree Splitting (10 pts)\n",
    "\n",
    "def make_tree(X, y):\n",
    "    # Return array if node is empty of pure (1 example on leaf node)\n",
    "    if y.shape[0] == 1 or y.shape[0] == 0:\n",
    "        return y\n",
    "    # Compute information gain for each feature\n",
    "    given_features_first = []\n",
    "    given_features_second = []\n",
    "\n",
    "    for i in range(0, X.shape[0]):\n",
    "        given_features_first.append(X[i][0])\n",
    "        given_features_second.append(X[i][1])\n",
    "    \n",
    "    given_features = np.array([given_features_first, given_features_second])\n",
    "\n",
    "    # Compute the information gain for each of the features\n",
    "    gains = []\n",
    "    for i in range(0, X.shape[1]):\n",
    "        gains.append(information_gain(given_features[i], y))\n",
    "\n",
    "    # Early stopping if there is no information gain\n",
    "    if (np.asarray(gains) <= 1e-05).all():\n",
    "        return y\n",
    "\n",
    "    # Else, get best feature\n",
    "    best_feature = np.argmax(gains)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Use the split function to split on the best feature\n",
    "    subset_dict = split(X[:, best_feature])\n",
    "\n",
    "    # Note that each entry in the dictionary returned by\n",
    "    # split is an attribute_value:array_indices pair.\n",
    "    # here, we are going to iterate over these key-value\n",
    "    # pairs and select the respective examples for the\n",
    "    # new child nodes\n",
    "\n",
    "    for feature_value, train_example_indices in subset_dict.items():\n",
    "        child_y_subset = np.array([y[i] for i in train_example_indices])\n",
    "        child_x_subset = np.array([X[i] for i in train_example_indices])\n",
    "\n",
    "        # Next, we are using \"recursion,\" that is, calling the same\n",
    "        # tree_split function on the child subset(s)\n",
    "        \n",
    "        results[\"X_%d = %d\" % (best_feature, feature_value)] = \\\n",
    "                make_tree(child_x_subset, child_y_subset)\n",
    "    \n",
    "    return results\n",
    "\n",
    "x1 = np.array([0, 0, 1, 1, 2, 2])\n",
    "x2 = np.array([0, 1, 0, 1, 0, 1])\n",
    "X = np.array([x1, x2]).T\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "print('Inputs:\\n', X)\n",
    "print('\\nLabels:\\n', y)\n",
    "print('\\nDecision tree:\\n', make_tree(X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1.1.5 1.5) Building a Decision Tree API (10 pts)\n",
    "import collections\n",
    "from collections import Counter\n",
    "class ID3DecisionTreeClassifer(object):\n",
    "    def __init__(self):\n",
    "        pass    \n",
    "    def fit(self, X, y):\n",
    "        self.splits_ = make_tree(X, y)\n",
    "    def _majority_vote(self, label_array):\n",
    "        counter = split(label_array)\n",
    "        max_vote = list(counter.keys())[0] \n",
    "        return str(max_vote)\n",
    "    def _traverse(self, x, d):\n",
    "        if isinstance(d, np.ndarray):\n",
    "            return d\n",
    "        for key in d:\n",
    "            name, value = key.split(' = ')\n",
    "            feature_idx = int(name.split('_')[-1])\n",
    "            value = int(value)\n",
    "            if x[feature_idx] == value:\n",
    "                return self._traverse(x, d[key])\n",
    "    def predict(self, x):\n",
    "        label_array = self._traverse(x, self.splits_)\n",
    "        return self._majority_vote(label_array)\n",
    "\n",
    "tree = ID3DecisionTreeClassifer()\n",
    "tree.fit(X, y)\n",
    "print(tree.predict(np.array([0, 0])))\n",
    "print(tree.predict(np.array([0, 1])))\n",
    "print(tree.predict(np.array([1, 0])))\n",
    "print(tree.predict(np.array([1, 0])))\n",
    "print(tree.predict(np.array([1, 1])))\n",
    "print(tree.predict(np.array([2, 0])))\n",
    "print(tree.predict(np.array([2, 1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 150\n",
      "Number of features: 4\n",
      "Unique class labels: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# 1.2.1 2.1 Bootstrapping (10 pts)\n",
    "\n",
    "from mlxtend.data import iris_data\n",
    "X, y = iris_data()\n",
    "print('Number of examples:', X.shape[0])\n",
    "print('Number of features:', X.shape[1])\n",
    "print('Unique class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 105\n",
      "Number of test examples: 45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 45, train_size = 105, random_state = 123, stratify = y)\n",
    "print('Number of training examples:', X_train.shape[0])\n",
    "print('Number of test examples:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training inputs from bootstrap round: 105\n",
      "Number of training labels from bootstrap round: 105\n",
      "Labels:\n",
      " [0 0 1 0 0 1 2 0 2 1 0 0 2 1 1 1 1 2 1 1 2 0 2 1 2 1 1 1 0 1 0 0 1 2 0 0 0\n",
      " 0 2 1 1 2 1 2 1 1 2 1 2 0 1 1 2 2 1 0 1 0 2 2 0 1 0 2 0 0 0 0 1 2 0 0 1 0\n",
      " 1 1 0 1 1 2 2 0 2 0 2 0 1 1 2 2 0 2 2 2 0 1 0 1 2 2 2 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def draw_bootstrap_sample(rng, X, y):\n",
    "    sample_indices = []\n",
    "    for i in range (0, 105):\n",
    "        sample_indices.append(i)\n",
    "    bootstrap_indices = rng.choice(sample_indices, len(sample_indices))\n",
    "    return X[bootstrap_indices], y[bootstrap_indices]\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "print('Number of training inputs from bootstrap round:', X_boot.shape[0])\n",
    "print('Number of training labels from bootstrap round:', y_boot.shape[0])\n",
    "print('Labels:\\n', y_boot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Tree Accuracies:\n",
      "88.9%\n",
      "93.3%\n",
      "97.8%\n",
      "93.3%\n",
      "93.3%\n",
      "93.3%\n",
      "91.1%\n",
      "97.8%\n",
      "97.8%\n",
      "97.8%\n",
      "\n",
      "Bagging Test Accuracy: 97.8%\n"
     ]
    }
   ],
   "source": [
    "# 1.2.2 2.2 Bagging classifier from decision trees\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "class BaggingClassifier(object):\n",
    "    def __init__(self, num_trees=10, random_state=123):\n",
    "        self.num_trees = num_trees\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "    def fit(self, X, y):\n",
    "        self.trees_ = [DecisionTreeClassifier(random_state=self.rng) for i in range(self.num_trees)]\n",
    "        for i in range(self.num_trees):\n",
    "            X_boot, y_boot = draw_bootstrap_sample(self.rng, X, y)\n",
    "            self.trees_[i] = self.trees_[i].fit(X_boot, y_boot)\n",
    "    def predict(self, X):   \n",
    "        ary = np.zeros((X.shape[0], len(self.trees_)), dtype=np.int)\n",
    "        for i in range(len(self.trees_)):\n",
    "            ary[:, i] = self.trees_[i].predict(X)\n",
    "            maj = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=1, arr=ary)\n",
    "        \n",
    "        return maj\n",
    "\n",
    "model = BaggingClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "print('Individual Tree Accuracies:')\n",
    "for tree in model.trees_:\n",
    "    predictions = tree.predict(X_test)\n",
    "    print('%.1f%%' % ((predictions == y_test).sum() / X_test.shape[0] * 100))\n",
    "print('\\nBagging Test Accuracy: %.1f%%' % ((predictions == y_test).sum() / X_test.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average bias: 0.022222222222222223\n"
     ]
    }
   ],
   "source": [
    "# 1.3.1 3.1 Bias-Variance decomposition of the 0-1 Loss for Decision Trees (10 pts)\n",
    "rng = np.random.RandomState(123)\n",
    "num_bootstrap = 200\n",
    "all_pred = np.zeros((num_bootstrap, y_test.shape[0]), dtype = np.int)\n",
    "for i in range(num_bootstrap):\n",
    "    X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "    pred = DecisionTreeClassifier(random_state = 66).fit(X_boot, y_boot).predict(X_test)\n",
    "    all_pred[i] = pred\n",
    "\n",
    "main_predictions = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis = 0, arr = all_pred)\n",
    "\n",
    "def calculate_bias(y_test, main_predictions):\n",
    "    bias = 0\n",
    "    for i in range(0, len(y_test)):\n",
    "        if (y_test[i] != main_predictions[i]):\n",
    "            bias = bias + y_test[i]\n",
    "    return bias / len(y_test)\n",
    "\n",
    "bias = calculate_bias(y_test, main_predictions)\n",
    "print ('Average bias:', bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average variance: 0.03333333333333333\n"
     ]
    }
   ],
   "source": [
    "def average_variance(all_pred, main_predictions):\n",
    "    variance = 0\n",
    "    for i in all_pred:\n",
    "        variance = np.mean([(sum(i != main_predictions)/len(i))])\n",
    "    return variance/2\n",
    "    # return np.mean([(sum(preds!=main_predictions)/len(preds)) for preds in all_pred])\n",
    "\n",
    "var = average_variance(all_pred, main_predictions)\n",
    "print ('Average variance:', var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average bias: 0.022222222222222223\n",
      "Average variance: 0.022222222222222223\n"
     ]
    }
   ],
   "source": [
    "# 1.3.2 3.2 Bias-Variance decomposition of the 0-1 Loss for Bagging (10 pts)\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "num_bootstrap = 200\n",
    "all_pred = np.zeros((num_bootstrap, y_test.shape[0]), dtype = np.int)\n",
    "\n",
    "bagging_classifier = BaggingClassifier()\n",
    "\n",
    "for i in range(num_bootstrap):\n",
    "    X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "    pred = bagging_classifier.fit(X_boot, y_boot).predict(X_test)\n",
    "    all_pred[i] = pred\n",
    "\n",
    "main_predictions = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis = 0, arr = all_pred)\n",
    "\n",
    "bias = calculate_bias(y_test, main_predictions)\n",
    "print ('Average bias:', bias)\n",
    "\n",
    "var = average_variance(all_pred, main_predictions)\n",
    "print ('Average variance:', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is the average variance higher or lower than the average of the decision tree in 3.1? And what about the average bias?** <br>\n",
    " The average variance is lower than as that in 3.1. Bagging helps to reduce variance since\n",
    " it's objective is the reduce the complexity of models that overfit the data.\n",
    " The average bias is the same as 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average bias: 0.06666666666666667\n",
      "Average variance: 0.022222222222222223\n"
     ]
    }
   ],
   "source": [
    "# 1.3.3 3.3 Bias-Variance decomposition of the 0-1 Loss for AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "num_bootstrap = 200\n",
    "all_pred = np.zeros((num_bootstrap, y_test.shape[0]), dtype = np.int)\n",
    "\n",
    "ada_classifier = AdaBoostClassifier()\n",
    "\n",
    "for i in range(num_bootstrap):\n",
    "    X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "    pred = ada_classifier.fit(X_boot, y_boot).predict(X_test)\n",
    "    all_pred[i] = pred\n",
    "\n",
    "main_predictions = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis = 0, arr = all_pred)\n",
    "\n",
    "bias = calculate_bias(y_test, main_predictions)\n",
    "print ('Average bias:', bias) # 0.06666666666666667\n",
    "\n",
    "var = average_variance(all_pred, main_predictions)\n",
    "print ('Average variance:', var) # 0.022222222222222223"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is the average variance higher or lower than the average of the decision tree in 3.1? And what\n",
    "about the average bias?** <br>\n",
    "The average variance is lower than the variance in 3.1. AdaBoost is robust to over fitting. The\n",
    "average bias is also higher. This may be because of the increase complexity of models that suffer from\n",
    "high bias due to under fitting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
